{"dataflow":{"dfKey":"4eac3edd-a384-4787-9a2d-0b71ffe2d544","name":"AWS_Iceberg","tags":null,"description":null,"definition":"{\"name\":\"AWS_Iceberg\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"Y\"},{\"code\":\"from pyspark.sql import SparkSession\\r\\n\\r\\n# AWS S3 access\\r\\naws_access_key \\u003d \\\"AKIAV5DLRL43YLKZ7EX2\\\"\\r\\naws_secret_key \\u003d \\\"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\\\"\\r\\ns3_bucket \\u003d \\\"datagapsqa1-target\\\"\\r\\nwarehouse_path \\u003d f\\\"s3a://{s3_bucket}/warehouse\\\"\\r\\ncsv_path \\u003d \\\"s3a://datagapsqa1-target/customers_2.csv.csv\\\"\\r\\n\\r\\n# Start Spark with Iceberg catalog pointing to AWS S3\\r\\nspark \\u003d SparkSession.builder \\\\\\r\\n    .appName(\\\"Write CSV to Iceberg on S3\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog\\\", \\\"org.apache.iceberg.spark.SparkCatalog\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog.type\\\", \\\"hadoop\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog.warehouse\\\", warehouse_path) \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.access.key\\\", aws_access_key) \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.secret.key\\\", aws_secret_key) \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.endpoint\\\", \\\"s3.amazonaws.com\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.path.style.access\\\", \\\"true\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.impl\\\", \\\"org.apache.hadoop.fs.s3a.S3AFileSystem\\\") \\\\\\r\\n    .getOrCreate()\\r\\n\\r\\n# Read CSV from AWS S3\\r\\ndf \\u003d spark.read.option(\\\"header\\\", \\\"true\\\").option(\\\"inferSchema\\\", \\\"true\\\").csv(csv_path)\\r\\n\\r\\n# Create Iceberg database (if not exists)\\r\\nspark.sql(\\\"CREATE DATABASE IF NOT EXISTS my_catalog.iceberg_db\\\")\\r\\n\\r\\n# Write data to Iceberg table in S3\\r\\ndf.writeTo(\\\"my_catalog.iceberg_db.customers\\\").createOrReplace()\\r\\ndf.show()\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":1,\"componentName\":\"S3_Iceberg\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from pyspark.sql import SparkSession\\r\\n\\r\\nspark \\u003d SparkSession.builder \\\\\\r\\n    .appName(\\\"Iceberg MinIO\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog\\\", \\\"org.apache.iceberg.spark.SparkCatalog\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog.catalog-impl\\\", \\\"org.apache.iceberg.hadoop.HadoopCatalog\\\") \\\\\\r\\n    .config(\\\"spark.sql.catalog.my_catalog.warehouse\\\", \\\"s3a://iceberg-warehouse/\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.endpoint\\\", \\\"http://192.168.6.91:9000\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.access.key\\\", \\\"minioadmin\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.secret.key\\\", \\\"minioadmin\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.path.style.access\\\", \\\"true\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.connection.ssl.enabled\\\", \\\"false\\\") \\\\\\r\\n    .config(\\\"spark.hadoop.fs.s3a.impl\\\", \\\"org.apache.hadoop.fs.s3a.S3AFileSystem\\\") \\\\\\r\\n    .getOrCreate()\\r\\n\\r\\n# â Confirm values\\r\\nconf \\u003d spark._jsc.hadoopConfiguration()\\r\\nprint(\\\"endpoint:\\\", conf.get(\\\"fs.s3a.endpoint\\\"))\\r\\nprint(\\\"access key:\\\", conf.get(\\\"fs.s3a.access.key\\\"))\\r\\nprint(\\\"secret key:\\\", conf.get(\\\"fs.s3a.secret.key\\\"))\\r\\nprint(\\\"path style access:\\\", conf.get(\\\"fs.s3a.path.style.access\\\"))\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"import boto3\\r\\n\\r\\ns3 \\u003d boto3.client(\\r\\n    \\u0027s3\\u0027,\\r\\n    endpoint_url\\u003d\\u0027http://192.168.6.91:9000\\u0027,\\r\\n    aws_access_key_id\\u003d\\u0027minioadmin\\u0027,\\r\\n    aws_secret_access_key\\u003d\\u0027minioadmin\\u0027\\r\\n)\\r\\n\\r\\nresponse \\u003d s3.list_objects_v2(Bucket\\u003d\\u0027iceberg-warehouse\\u0027)\\r\\nprint(response)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":3,\"componentName\":\"Code 3\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"type\":\"static\",\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":3,"livyOptions":null,"isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow","workSchemaName":null,"diagramSchema":"{\"isCustom\":false,\"diagramDef\":[]}"},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[],"filterDatasetMappings":[],"referenceDatasets":null}