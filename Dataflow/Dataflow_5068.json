{"dataflow":{"dfKey":"ded6768b-1cab-445d-b4d7-71999b1454a4","name":"Dataflow_5068","tags":null,"description":null,"definition":"{\"name\":\"Dataflow_5068\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"Y\"},{\"code\":\"import json\\r\\nimport pandas as pd\\r\\n\\r\\nimport pandas as pd\\r\\nfrom pyspark.sql import SparkSession\\r\\n\\r\\nfile_name \\u003d \\u0027customer_sales_data.json\\u0027\\r\\nfile_path \\u003d f\\u0027/home/centos/data/{file_name}\\u0027\\r\\n\\r\\n# Step 1: Read the JSON file\\r\\nwith open(file_path, \\u0027r\\u0027) as fp:\\r\\n    data \\u003d json.load(fp)\\r\\n    print(\\\"ð¦ Raw JSON data loaded successfully.\\\\n\\\")\\r\\n\\r\\n# Step 2: Flatten function\\r\\ndef flatten_json(y, parent_key\\u003d\\u0027\\u0027, sep\\u003d\\u0027_\\u0027):\\r\\n    items \\u003d {}\\r\\n    for k, v in y.items():\\r\\n        new_key \\u003d f\\\"{parent_key}{sep}{k}\\\" if parent_key else k\\r\\n        if isinstance(v, dict):\\r\\n            items.update(flatten_json(v, new_key, sep\\u003dsep))\\r\\n        else:\\r\\n            items[new_key] \\u003d v\\r\\n    return items\\r\\n\\r\\n# Step 3: Flatten JSON and convert to DataFrame\\r\\nflattened_data \\u003d [flatten_json(record) for record in data]\\r\\ndf \\u003d pd.DataFrame(flattened_data)\\r\\n\\r\\nprint(\\\"ð Flattened Student Table:\\\\n\\\")\\r\\nprint(df.to_string(index\\u003dFalse))\\r\\n\\r\\n# Step 4: Dataset name\\r\\n\\r\\n\\r\\n\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"format\":\"json\",\"enableSchema\":\"N\",\"schema\":[],\"fileMetadata\":[],\"enableTrim\":\"N\",\"failComponentWhenDatasetEmpty\":\"N\",\"componentId\":2,\"componentName\":\"File 2\",\"tableName\":\"File_2\",\"category\":\"Source\",\"componentType\":\"File\",\"rank\":0,\"dataSourceName\":\"ADLS_gen2_JSON\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"mode\":\"PERMISSIVE\",\"dateFormat\":\"yyyy-MM-dd\",\"multiLine\":\"false\",\"timestampFormat\":\"yyyy-MM-dd \\u0027T\\u0027 HH:mm:ss.SSSXXX\",\"inferSchema\":\"true\",\"path\":\"POC/sales.json\"},\"className\":\"com.datagaps.dataflow.models.FileComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"\\nfrom datagaps_utilities import dataset_flatten_v2\\ndataset_flatten_v2(session_object\\u003dspark, input_dataset\\u003d\\\"File_2\\\", output_dataset_primary\\u003d\\\"File_2_result\\\", non_object_list_as_columns\\u003dFalse, use_short_names\\u003dFalse)\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"componentId\":3,\"componentName\":\"Code 3\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[2],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"allparentpassorfail\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"type\":\"static\",\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":4,"livyOptions":null,"isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow","workSchemaName":null,"diagramSchema":"{\"isCustom\":false,\"diagramDef\":[]}"},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[],"filterDatasetMappings":[],"referenceDatasets":null}