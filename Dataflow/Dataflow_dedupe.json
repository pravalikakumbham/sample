{"dataflow":{"dfKey":"37771e1a-3619-4467-a3be-a407049ed5c6","name":"Dataflow_dedupe","tags":null,"description":null,"definition":"{\"name\":\"Dataflow_dedupe\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"Y\"},{\"code\":\"import csv\\r\\nimport logging\\r\\nimport optparse\\r\\nimport os\\r\\nimport re\\r\\n\\r\\nimport dedupe\\r\\nfrom unidecode import unidecode\\r\\n\\r\\n\\r\\ndef preProcess(column):\\r\\n    column \\u003d unidecode(column)\\r\\n    column \\u003d re.sub(\\\"  +\\\", \\\" \\\", column)\\r\\n    column \\u003d re.sub(\\\"\\\\n\\\", \\\" \\\", column)\\r\\n    column \\u003d column.strip().strip(\\u0027\\\"\\u0027).strip(\\\"\\u0027\\\").lower().strip()\\r\\n\\r\\n    if not column:\\r\\n        column \\u003d None\\r\\n    return column\\r\\n\\r\\n\\r\\ndef readData(filename):\\r\\n    data_d \\u003d {}\\r\\n    with open(filename) as f:\\r\\n        reader \\u003d csv.DictReader(f)\\r\\n        for row in reader:\\r\\n            clean_row \\u003d [(k, preProcess(v)) for (k, v) in row.items()]\\r\\n            row_id \\u003d int(row[\\\"id\\\"])\\r\\n            data_d[row_id] \\u003d dict(clean_row)\\r\\n\\r\\n    return data_d\\r\\n\\r\\n\\r\\nif __name__ \\u003d\\u003d \\\"__main__\\\":\\r\\n    optp \\u003d optparse.OptionParser()\\r\\n    optp.add_option(\\r\\n        \\\"-v\\\",\\r\\n        \\\"--verbose\\\",\\r\\n        dest\\u003d\\\"verbose\\\",\\r\\n        action\\u003d\\\"count\\\",\\r\\n        help\\u003d\\\"Increase verbosity (specify multiple times for more)\\\",\\r\\n    )\\r\\n    (opts, args) \\u003d optp.parse_args()\\r\\n    log_level \\u003d logging.WARNING\\r\\n    if opts.verbose:\\r\\n        if opts.verbose \\u003d\\u003d 1:\\r\\n            log_level \\u003d logging.INFO\\r\\n        elif opts.verbose \\u003e\\u003d 2:\\r\\n            log_level \\u003d logging.DEBUG\\r\\n    logging.basicConfig(level\\u003dlog_level)\\r\\n\\r\\n    input_file \\u003d \\\"people.csv\\\"\\r\\n    settings_file \\u003d \\\"csv_example_learned_settings\\\"\\r\\n    training_file \\u003d \\\"csv_example_training.json\\\"\\r\\n\\r\\n    print(\\\"importing data ...\\\")\\r\\n    data_d \\u003d readData(input_file)\\r\\n\\r\\n    if os.path.exists(settings_file):\\r\\n        print(\\\"reading from\\\", settings_file)\\r\\n        with open(settings_file, \\\"rb\\\") as f:\\r\\n            deduper \\u003d dedupe.StaticDedupe(f)\\r\\n    else:\\r\\n        fields \\u003d [\\r\\n            dedupe.variables.String(\\\"name\\\"),\\r\\n            #dedupe.variables.String(\\\"email\\\"),\\r\\n            #dedupe.variables.String(\\\"phone\\\", has_missing\\u003dTrue)\\r\\n        ]\\r\\n\\r\\n        deduper \\u003d dedupe.Dedupe(fields)\\r\\n\\r\\n        if os.path.exists(training_file):\\r\\n            print(\\\"reading labeled examples from \\\", training_file)\\r\\n            with open(training_file, \\\"rb\\\") as f:\\r\\n                deduper.prepare_training(data_d, f)\\r\\n        else:\\r\\n            deduper.prepare_training(data_d)\\r\\n\\r\\n            print(\\\"starting active labeling...\\\")\\r\\n            dedupe.console_label(deduper)\\r\\n\\r\\n            deduper.train()\\r\\n\\r\\n            with open(training_file, \\\"w\\\") as tf:\\r\\n                deduper.write_training(tf)\\r\\n\\r\\n            with open(settings_file, \\\"wb\\\") as sf:\\r\\n                deduper.write_settings(sf)\\r\\n\\r\\n    print(\\\"clustering...\\\")\\r\\n    clustered_dupes \\u003d deduper.partition(data_d, 0.5)\\r\\n\\r\\n    print(\\\"# duplicate sets\\\", len(clustered_dupes))\\r\\n\\r\\n    cluster_membership \\u003d {}\\r\\n    for cluster_id, (records, scores) in enumerate(clustered_dupes):\\r\\n        for record_id, score in zip(records, scores):\\r\\n            cluster_membership[record_id] \\u003d {\\r\\n                \\\"Cluster ID\\\": cluster_id,\\r\\n                \\\"confidence_score\\\": score,\\r\\n            }\\r\\n\\r\\n    # Calculate cluster sizes to differentiate unique vs duplicate\\r\\n    cluster_sizes \\u003d {cluster_id: len(records) for cluster_id, (records, scores) in enumerate(clustered_dupes)}\\r\\n\\r\\n    unique_records \\u003d []\\r\\n    duplicate_records \\u003d []\\r\\n\\r\\n    # Read input CSV again and split records\\r\\n    with open(input_file) as f_input:\\r\\n        reader \\u003d csv.DictReader(f_input)\\r\\n        for row in reader:\\r\\n            row_id \\u003d int(row[\\\"id\\\"])\\r\\n            cluster_info \\u003d cluster_membership.get(row_id, None)\\r\\n            if cluster_info:\\r\\n                cluster_id \\u003d cluster_info[\\\"Cluster ID\\\"]\\r\\n                if cluster_sizes[cluster_id] \\u003d\\u003d 1:\\r\\n                    unique_records.append((row, cluster_info))\\r\\n                else:\\r\\n                    duplicate_records.append((row, cluster_info))\\r\\n            else:\\r\\n                # Not clustered means unique or unknown, treat as unique here\\r\\n                unique_records.append((row, None))\\r\\n\\r\\n    # Write unique records to CSV\\r\\n    with open(\\\"unique_records.csv\\\", \\\"w\\\", newline\\u003d\\u0027\\u0027) as unique_out:\\r\\n        fieldnames \\u003d [\\\"Cluster ID\\\", \\\"confidence_score\\\"] + reader.fieldnames\\r\\n        writer \\u003d csv.DictWriter(unique_out, fieldnames\\u003dfieldnames)\\r\\n        writer.writeheader()\\r\\n        for row, info in unique_records:\\r\\n            if info:\\r\\n                row.update(info)\\r\\n            else:\\r\\n                row.update({\\\"Cluster ID\\\": \\\"\\\", \\\"confidence_score\\\": \\\"\\\"})\\r\\n            writer.writerow(row)\\r\\n\\r\\n    # Write duplicate records to CSV\\r\\n    with open(\\\"duplicate_records.csv\\\", \\\"w\\\", newline\\u003d\\u0027\\u0027) as duplicate_out:\\r\\n        fieldnames \\u003d [\\\"Cluster ID\\\", \\\"confidence_score\\\"] + reader.fieldnames\\r\\n        writer \\u003d csv.DictWriter(duplicate_out, fieldnames\\u003dfieldnames)\\r\\n        writer.writeheader()\\r\\n        for row, info in duplicate_records:\\r\\n            if info:\\r\\n                row.update(info)\\r\\n            else:\\r\\n                row.update({\\\"Cluster ID\\\": \\\"\\\", \\\"confidence_score\\\": \\\"\\\"})\\r\\n            writer.writerow(row)\\r\\n\\r\\n    print(\\\"Wrote unique records to unique_records.csv\\\")\\r\\n    print(\\\"Wrote duplicate records to duplicate_records.csv\\\")\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"type\":\"static\",\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":1,"livyOptions":null,"isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow","workSchemaName":null,"diagramSchema":"{\"isCustom\":false,\"diagramDef\":[]}"},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[],"filterDatasetMappings":[],"referenceDatasets":null}