{"dataflow":{"dfKey":"3ff20231-44fb-4829-8ea1-e1e1305dc8aa","name":"Datasource_plugin_worked","tags":null,"description":null,"definition":"{\"name\":\"Datasource_plugin_worked\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"Y\"},{\"code\":\"\\r\\nimport os\\r\\nimport shutil\\r\\nimport json\\r\\nimport datetime\\r\\n\\r\\nfrom py4j.java_gateway import java_import\\r\\nfrom datagaps_utilities.config import properties_file_path\\r\\nfrom jproperties import Properties\\r\\nimport pandas as pd\\r\\nimport requests\\r\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\r\\n\\r\\n# Import Java classes for Spark-JVM interaction\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\nconfigs \\u003d Properties()\\r\\n\\r\\nbearer_token \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getPasswordByKeyAndContainerIdFromWallet(\\\"token\\\", 715)\\r\\n\\r\\nprint(bearer_token)\\r\\n\\r\\n\\r\\ndef Getdatasource_Details():\\r\\n    conn_params \\u003d {}\\r\\n    with open(properties_file_path, \\\"rb\\\") as config_file:\\r\\n        configs.load(config_file)\\r\\n        conn_params[\\\"user\\\"] \\u003d configs[\\\"datasource.userName\\\"].data\\r\\n        conn_params[\\\"password\\\"] \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(\\r\\n            configs[\\\"datasource.password\\\"].data\\r\\n        )\\r\\n        conn_params[\\\"url\\\"] \\u003d configs[\\\"datasource.jdbcUrl\\\"].data\\r\\n\\r\\n    query \\u003d f\\\"\\\"\\\"\\r\\n        SELECT *\\r\\n        FROM public.t_data_source\\r\\n        WHERE dstype\\u003d\\u0027jdbc\\u0027 \\r\\n          AND container_id\\u003d{715} \\r\\n          AND format\\u003d\\u0027jdbc\\u0027 \\r\\n          AND is_workschema\\u003d\\u0027N\\u0027\\r\\n    \\\"\\\"\\\"\\r\\n\\r\\n    df \\u003d spark.read.format(\\\"jdbc\\\") \\\\\\r\\n        .option(\\\"url\\\", conn_params[\\\"url\\\"]) \\\\\\r\\n        .option(\\\"user\\\", conn_params[\\\"user\\\"]) \\\\\\r\\n        .option(\\\"password\\\", conn_params[\\\"password\\\"]) \\\\\\r\\n        .option(\\\"query\\\", query).load()\\r\\n\\r\\n    data_source_details \\u003d df.toPandas()\\r\\n\\r\\n    \\r\\n    \\r\\n    application_url \\u003d \\\"https://poc.datagaps.com\\\" \\r\\n\\r\\n    if application_url.endswith(\\u0027/\\u0027):\\r\\n        application_url \\u003d application_url[:-1]\\r\\n\\r\\n    def check_connection(row):\\r\\n        data_source_id \\u003d int(row[\\u0027data_src_id\\u0027])\\r\\n        url \\u003d application_url + f\\\"/CoreServiceRest/api/v1.0/savedQueries/datasource/{data_source_id}/query?fetchSize\\u003d50\\u0026runCount\\u003dfalse\\\"\\r\\n        \\r\\n        # Updated: actual query in JSON format\\r\\n        payload \\u003d {\\r\\n            \\\"query\\\": \\\"select \\u0027hello\\u0027 as first_name\\\"\\r\\n        }\\r\\n\\r\\n        headers \\u003d {\\r\\n            \\u0027Content-Type\\u0027: \\u0027application/json\\u0027,  # Changed to application/json\\r\\n            \\u0027Authorization\\u0027: f\\u0027Bearer {bearer_token}\\u0027\\r\\n        }\\r\\n\\r\\n        try:\\r\\n            response \\u003d requests.put(url, headers\\u003dheaders, data\\u003djson.dumps(payload), timeout\\u003d30)\\r\\n\\r\\n            if response.status_code in [200, 201, 204, 500]:\\r\\n                try:\\r\\n                    query_json \\u003d response.json()\\r\\n\\r\\n                    if isinstance(query_json, dict):\\r\\n                        if query_json.get(\\\"status\\\") \\u003d\\u003d 500:\\r\\n                            error_msg \\u003d query_json.get(\\\"message\\\", \\\"\\\")\\r\\n                            details_list \\u003d query_json.get(\\\"details\\\", [])\\r\\n                            details_str \\u003d \\\"; \\\".join(\\r\\n                                [str(d) if d is not None else \\\"\\\" for d in details_list]\\r\\n                            ) or \\\"No details\\\"\\r\\n\\r\\n                            full_text \\u003d f\\\"{error_msg} {details_str}\\\".lower()\\r\\n                            if \\\"from keyword not found where expected\\\" in full_text:\\r\\n                                return (\\\"Success\\\", f\\\"Connection got success, query is wrong - {error_msg} - {details_str}\\\")\\r\\n                            else:\\r\\n                                return (\\\"Success\\\", f\\\"Query Failure - 500 - {error_msg} - {details_str}\\\")\\r\\n                        else:\\r\\n                            return (\\\"Success\\\", \\\"Success\\\")\\r\\n\\r\\n                    elif isinstance(query_json, list):\\r\\n                        return (\\\"Success\\\", \\\"Success\\\")\\r\\n                    else:\\r\\n                        return (\\\"Success\\\", \\\"Success\\\")\\r\\n\\r\\n                except ValueError:\\r\\n                    return (\\\"Success\\\", \\\"Success\\\")\\r\\n\\r\\n            else:\\r\\n                return (f\\\"Failure\\\", \\\"Query Not Executed (Connection Failed)\\\")\\r\\n\\r\\n        except Exception as e:\\r\\n            return (f\\\"Failure\\\", \\\"Query Not Executed (Connection Failed)\\\")\\r\\n\\r\\n    connection_statuses_ordered \\u003d [None] * len(data_source_details)\\r\\n    query_statuses_ordered \\u003d [None] * len(data_source_details)\\r\\n\\r\\n    with ThreadPoolExecutor(max_workers\\u003d10) as executor:\\r\\n        futures \\u003d {\\r\\n            executor.submit(check_connection, row): idx\\r\\n            for idx, (_, row) in enumerate(data_source_details.iterrows())\\r\\n        }\\r\\n\\r\\n        for future in as_completed(futures):\\r\\n            idx \\u003d futures[future]\\r\\n            conn_status, query_status \\u003d future.result()\\r\\n            connection_statuses_ordered[idx] \\u003d conn_status\\r\\n            query_statuses_ordered[idx] \\u003d query_status\\r\\n\\r\\n    data_source_details[\\\"connection_status\\\"] \\u003d connection_statuses_ordered\\r\\n    data_source_details[\\\"query_status\\\"] \\u003d query_statuses_ordered\\r\\n    \\r\\n    print(data_source_details.head())\\r\\n\\r\\n    return data_source_details[[\\u0027data_src_id\\u0027,\\u0027name\\u0027,\\u0027source_type\\u0027,\\u0027connection_status\\u0027]]\\r\\n\\r\\n\\r\\ndef create_spark_datasets(results, dataflow_run_id, component_name):\\r\\n\\r\\n    spark_df \\u003d spark.createDataFrame(results)\\r\\n\\r\\n    spark_df.show()\\r\\n\\r\\n    dataset_name \\u003d \\\"datasource_statuses\\\"+\\\"$[component_name]\\\".replace(\\u0027 \\u0027,\\u0027_\\u0027)\\r\\n\\r\\n    spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(\\r\\n        json.dumps([dataset_name]), dataflow_run_id, component_name\\r\\n    )\\r\\n\\r\\n    spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(\\r\\n        spark_df._jdf, dataset_name, component_name, dataflow_run_id\\r\\n    )\\r\\n\\r\\n    spark_df.createOrReplaceTempView(dataset_name)\\r\\n\\r\\n\\r\\ndef main():\\r\\n    results \\u003d Getdatasource_Details()\\r\\n    dataflow_run_id \\u003d $[dataflow_run_id]\\r\\n    component_name \\u003d \\\"$[component_name]\\\"\\r\\n    create_spark_datasets(results, dataflow_run_id, component_name)\\r\\n\\r\\n\\r\\nmain()\\r\\n\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"pluginId\":\"b4f80c7a-8cc9-4fa6-b25c-ccc3b404982b\",\"pluginName\":\"Datasource_connection\",\"pluginOptions\":{\"parameters\":[{\"parameterId\":\"913458ac-ee22-46d1-8c53-1ff38bdfdad3\",\"fieldName\":\"application_bearer_token\",\"displayName\":\"Provide the vault key of bearer token\",\"value\":\"token\",\"dataType\":\"String\"}],\"inputDatasets\":[],\"outputDatasets\":[],\"dataSources\":[],\"additionalProps\":[]},\"hideDatasets\":\"N\",\"componentId\":3,\"componentName\":\"plugin worked\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Plugin\",\"rank\":0,\"displayRows\":0,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.PluginComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"type\":\"static\",\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":4,"livyOptions":null,"isDeleted":"N","userName":null,"type":"dataflow","environmentName":"","folderPath":"Dataflow/Data Source","workSchemaName":null,"diagramSchema":"{\"isCustom\":false,\"diagramDef\":[]}"},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[],"filterDatasetMappings":[],"referenceDatasets":"{\"3\":[]}"}