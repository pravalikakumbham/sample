{"dataflow":{"dfKey":"c25163a8-df1b-4274-974d-a65192cd669b","name":"Dataflow_3517","tags":null,"description":null,"definition":"{\"name\":\"Dataflow_3517\",\"engineVariableName\":\"\",\"components\":[{\"udfNames\":[],\"componentId\":0,\"componentName\":\"startComponent\",\"tableName\":\"\",\"category\":\"Start\",\"componentType\":\"UDF\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":0,\"dependencies\":[],\"className\":\"com.datagaps.dataflow.models.UDFComponent\",\"executionOption\":\"\",\"excludeNotification\":\"Y\"},{\"code\":\"from pyspark.sql.functions import regexp_extract, col\\r\\n\\r\\njdbc_url \\u003d \\\"jdbc:postgresql://10.0.1.132:5432/poc?prepareThreshold\\u003d0\\u0026default_transaction_read_only\\u003dfalse\\u0026OpenSourceSubProtocolOverride\\u003dtrue\\u0026stringtype\\u003dunspecified\\\" \\r\\n\\r\\nproperties \\u003d {\\\"user\\\": \\\"postgres\\\", \\\"password\\\": \\\"postgres\\\", \\\"driver\\\": \\\"org.postgresql.Driver\\\"}\\r\\n\\r\\nrepo_df \\u003d spark.read.jdbc(url\\u003djdbc_url, table\\u003d\\\"t_dm_data_rule_run\\\", properties\\u003dproperties)\\r\\nbad_tables_df \\u003d repo_df.filter(repo_df.bad_records_count \\u003e 0)\\r\\n#bad_tables_df.show(30)\\r\\n\\r\\nbad_tables_with_names_df \\u003d bad_tables_df.withColumn(\\r\\n    \\\"table_name\\\",\\r\\n    regexp_extract(col(\\\"result_count_query\\\"), \\\"(?i)(?:from|join)\\\\\\\\s+([a-zA-Z0-9_.]+)\\\", 1)\\r\\n)\\r\\n\\r\\n# Show the extracted table names\\r\\nbad_tables_with_names_df.select(\\\"table_name\\\").show(100)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":1,\"componentName\":\"Code 1\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\nimport psycopg2\\r\\nimport pandas as pd\\r\\n\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\noutput_dataset \\u003d \\\"bad_records_output\\\"\\r\\n\\r\\nconn_params \\u003d {\\r\\n    \\\"host\\\": \\\"10.0.1.132\\\",\\r\\n    \\\"port\\\": 5432,\\r\\n    \\\"database\\\": \\\"poc\\\",\\r\\n    \\\"user\\\": \\\"postgres\\\",\\r\\n    \\\"password\\\": \\\"postgres\\\"\\r\\n}\\r\\n\\r\\n# Your original query unchanged\\r\\nquery \\u003d \\\"\\\"\\\"\\r\\nWITH TableRowCounts AS (\\r\\n    SELECT \\r\\n        dt.table_id,\\r\\n        dt.schema_name,\\r\\n        dt.table_name,\\r\\n        c.reltuples::bigint AS current_row_count\\r\\n    FROM \\r\\n        t_dm_table dt\\r\\n    JOIN \\r\\n        pg_class c ON c.relname \\u003d dt.table_name\\r\\n    JOIN \\r\\n        pg_namespace n ON n.oid \\u003d c.relnamespace AND n.nspname \\u003d dt.schema_name\\r\\n)\\r\\n\\r\\nSELECT \\r\\n    dr_run.data_model_rule_run_id,\\r\\n    dr_run.bad_records_count,\\r\\n    dt.schema_name,\\r\\n    dt.table_name,\\r\\n    dr.column_id,\\r\\n    dr.rule_name,\\r\\n    dr_run.start_time,\\r\\n    dr_run.end_time\\r\\nFROM \\r\\n    t_dm_data_rule_run dr_run\\r\\nJOIN \\r\\n    t_dm_data_rule dr ON dr_run.data_model_rule_id \\u003d dr.data_rule_id\\r\\nJOIN \\r\\n    t_dm_table dt ON dr.table_id \\u003d dt.table_id\\r\\nLEFT JOIN\\r\\n    TableRowCounts trc ON dt.table_id \\u003d trc.table_id\\r\\nWHERE \\r\\n    dr_run.bad_records_count \\u003e 0\\r\\nORDER BY\\r\\n    dr_run.start_time DESC;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\n# Minimal deletion info query\\r\\ndeletion_query \\u003d \\\"\\\"\\\"\\r\\nSELECT schema_name, table_name, is_deleted, deleted_level\\r\\nFROM t_dm_TABLE\\r\\nWHERE is_deleted IS NOT NULL;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\nwith psycopg2.connect(**conn_params) as conn:\\r\\n    # Load original data\\r\\n    df \\u003d pd.read_sql(query, conn)\\r\\n    # Load deletion info\\r\\n    deletion_df \\u003d pd.read_sql(deletion_query, conn)\\r\\n\\r\\n    # Join on schema_name and table_name without changing types\\r\\n    df \\u003d df.merge(deletion_df, on\\u003d[\\u0027schema_name\\u0027, \\u0027table_name\\u0027], how\\u003d\\u0027left\\u0027)\\r\\n    \\r\\n    # Replace NaNs in deletion columns with None for cleaner Spark handling\\r\\n    df[\\u0027is_deleted\\u0027] \\u003d df[\\u0027is_deleted\\u0027].where(pd.notnull(df[\\u0027is_deleted\\u0027]), None)\\r\\n    df[\\u0027deleted_level\\u0027] \\u003d df[\\u0027deleted_level\\u0027].where(pd.notnull(df[\\u0027deleted_level\\u0027]), None)\\r\\n\\r\\n    # Create Spark DataFrame letting Spark infer schema naturally\\r\\n    spark_df \\u003d spark.createDataFrame(df)\\r\\n    spark_df.createOrReplaceTempView(output_dataset)\\r\\n    spark_df.show(25)\\r\\n\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(\\r\\n        spark_df._jdf, output_dataset, \\u0027$[component_name]\\u0027, $[dataflow_run_id]\\r\\n)\\r\\n\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(\\r\\n    json.dumps(output_dataset), $[dataflow_run_id], \\\"$[component_name]\\\"\\r\\n)\\r\\n\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":2,\"componentName\":\"Code 2\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\nimport psycopg2\\r\\nimport pandas as pd\\r\\n\\r\\noutput_dataset \\u003d \\\"bad_records\\\"\\r\\n\\r\\n# Connection details\\r\\nconn_params \\u003d {\\r\\n    \\\"host\\\": \\\"10.0.1.132\\\",\\r\\n    \\\"port\\\": 5432,\\r\\n    \\\"database\\\": \\\"poc\\\",\\r\\n    \\\"user\\\": \\\"postgres\\\",\\r\\n    \\\"password\\\": \\\"postgres\\\"\\r\\n}\\r\\n\\r\\nquery \\u003d \\\"\\\"\\\"\\r\\nWITH TableRowCounts AS (\\r\\n    SELECT \\r\\n        dt.table_id,\\r\\n        dt.schema_name,\\r\\n        dt.table_name,\\r\\n        c.reltuples::bigint AS current_row_count\\r\\n    FROM \\r\\n        t_dm_table dt\\r\\n    JOIN \\r\\n        pg_class c ON c.relname \\u003d dt.table_name\\r\\n    JOIN \\r\\n        pg_namespace n ON n.oid \\u003d c.relnamespace AND n.nspname \\u003d dt.schema_name\\r\\n)\\r\\n\\r\\nSELECT \\r\\n    dr_run.data_model_rule_run_id,\\r\\n    dr_run.bad_records_count,\\r\\n    dt.schema_name,\\r\\n    dt.table_name,\\r\\n    trc.current_row_count,\\r\\n    dr.column_id,\\r\\n    dr.rule_name,\\r\\n    dr_run.start_time,\\r\\n    dr_run.end_time\\r\\nFROM \\r\\n    t_dm_data_rule_run dr_run\\r\\nJOIN \\r\\n    t_dm_data_rule dr ON dr_run.data_model_rule_id \\u003d dr.data_rule_id\\r\\nJOIN \\r\\n    t_dm_table dt ON dr.table_id \\u003d dt.table_id\\r\\nLEFT JOIN\\r\\n    TableRowCounts trc ON dt.table_id \\u003d trc.table_id\\r\\nWHERE \\r\\n    dr_run.bad_records_count \\u003e 0\\r\\nORDER BY\\r\\n    dr_run.start_time DESC;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\nwith psycopg2.connect(**conn_params) as conn:\\r\\n    df \\u003d pd.read_sql(query, conn)\\r\\n    spark_df \\u003d spark.createDataFrame(df)\\r\\n    spark_df.createOrReplaceTempView(output_dataset)\\r\\n    spark_df.show(25)\\r\\n \\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(\\r\\n        spark_df._jdf, output_dataset, \\u0027$[component_name]\\u0027, $[dataflow_run_id])\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":5,\"componentName\":\"Code 5\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\nimport psycopg2\\r\\nimport pandas as pd\\r\\nimport os\\r\\nimport json\\r\\n\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\noutput_dataset \\u003d \\\"bad_records_output_1\\\"\\r\\n\\r\\nconn_params \\u003d {\\r\\n    \\\"host\\\": \\\"10.0.1.132\\\",\\r\\n    \\\"port\\\": 5432,\\r\\n    \\\"database\\\": \\\"poc\\\",\\r\\n    \\\"user\\\": \\\"postgres\\\",\\r\\n    \\\"password\\\": \\\"postgres\\\"\\r\\n}\\r\\n\\r\\n# Your original query unchanged\\r\\nquery \\u003d \\\"\\\"\\\"\\r\\nWITH TableRowCounts AS (\\r\\n    SELECT \\r\\n        dt.table_id,\\r\\n        dt.schema_name,\\r\\n        dt.table_name,\\r\\n        c.reltuples::bigint AS current_row_count\\r\\n    FROM \\r\\n        t_dm_table dt\\r\\n    JOIN \\r\\n        pg_class c ON c.relname \\u003d dt.table_name\\r\\n    JOIN \\r\\n        pg_namespace n ON n.oid \\u003d c.relnamespace AND n.nspname \\u003d dt.schema_name\\r\\n)\\r\\n\\r\\nSELECT \\r\\n    dr_run.data_model_rule_run_id,\\r\\n    dr_run.bad_records_count,\\r\\n    dt.schema_name,\\r\\n    dt.table_name,\\r\\n    dr.column_id,\\r\\n    dr.rule_name,\\r\\n    dr_run.start_time,\\r\\n    dr_run.end_time\\r\\nFROM \\r\\n    t_dm_data_rule_run dr_run\\r\\nJOIN \\r\\n    t_dm_data_rule dr ON dr_run.data_model_rule_id \\u003d dr.data_rule_id\\r\\nJOIN \\r\\n    t_dm_table dt ON dr.table_id \\u003d dt.table_id\\r\\nLEFT JOIN\\r\\n    TableRowCounts trc ON dt.table_id \\u003d trc.table_id\\r\\nWHERE \\r\\n    dr_run.bad_records_count \\u003e 0\\r\\nORDER BY\\r\\n    dr_run.start_time DESC;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\n# Minimal deletion info query\\r\\ndeletion_query \\u003d \\\"\\\"\\\"\\r\\nSELECT schema_name, table_name, is_deleted, deleted_level\\r\\nFROM t_dm_TABLE\\r\\nWHERE is_deleted IS NOT NULL;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\nwith psycopg2.connect(**conn_params) as conn:\\r\\n    # Load original data\\r\\n    df \\u003d pd.read_sql(query, conn)\\r\\n    # Load deletion info\\r\\n    deletion_df \\u003d pd.read_sql(deletion_query, conn)\\r\\n\\r\\n    # Join on schema_name and table_name without changing types\\r\\n    df \\u003d df.merge(deletion_df, on\\u003d[\\u0027schema_name\\u0027, \\u0027table_name\\u0027], how\\u003d\\u0027left\\u0027)\\r\\n    \\r\\n    # Replace NaNs in deletion columns with None for cleaner Spark handling\\r\\n    df[\\u0027is_deleted\\u0027] \\u003d df[\\u0027is_deleted\\u0027].where(pd.notnull(df[\\u0027is_deleted\\u0027]), None)\\r\\n    df[\\u0027deleted_level\\u0027] \\u003d df[\\u0027deleted_level\\u0027].where(pd.notnull(df[\\u0027deleted_level\\u0027]), None)\\r\\n\\r\\n    # Create Spark DataFrame letting Spark infer schema naturally\\r\\n    spark_df \\u003d spark.createDataFrame(df)\\r\\n    spark_df.createOrReplaceTempView(output_dataset)\\r\\n    spark_df.show(25)\\r\\n\\r\\n# Save dataset with existing logic\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(\\r\\n        spark_df._jdf, output_dataset, \\u0027$[component_name]\\u0027, $[dataflow_run_id]\\r\\n)\\r\\n\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(\\r\\n    json.dumps(output_dataset), $[dataflow_run_id], \\\"$[component_name]\\\"\\r\\n)\\r\\noutput_file \\u003d r\\\"/tmp/bad_records_output.csv\\\"\\r\\n\\r\\n# Create folder if it doesn\\u0027t exist\\r\\nos.makedirs(os.path.dirname(output_file), exist_ok\\u003dTrue)\\r\\n\\r\\n# Save Pandas DataFrame directly\\r\\ndf.to_csv(output_file, index\\u003dFalse)\\r\\n\\r\\nprint(f\\\"CSV written successfully to: {output_file}\\\")\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":6,\"componentName\":\"Code 6\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"format\":\"csv\",\"enableSchema\":\"N\",\"schema\":[],\"fileMetadata\":[],\"enableTrim\":\"N\",\"failComponentWhenDatasetEmpty\":\"N\",\"maskDataColumns\":\"\",\"componentId\":7,\"componentName\":\"File 7\",\"tableName\":\"File_7\",\"category\":\"Source\",\"componentType\":\"File\",\"rank\":0,\"dataSourceName\":\"CSV_tmp_local\",\"displayRows\":50,\"dependencies\":[],\"options\":{\"delimiter\":\",\",\"multiLine\":\"true\",\"header\":\"true\",\"inferSchema\":\"true\",\"path\":\"bad_records_output.csv\"},\"className\":\"com.datagaps.dataflow.models.FileComponent\",\"isCheckpointEnabled\":\"N\",\"dataSourceLogicalName\":\"\",\"executionOption\":\"\",\"excludeNotification\":\"N\"},{\"code\":\"from py4j.java_gateway import java_import\\r\\nimport psycopg2\\r\\nimport pandas as pd\\r\\nimport os\\r\\nimport json\\r\\nimport io\\r\\nimport requests\\r\\nfrom urllib.parse import quote\\r\\n\\r\\njava_import(spark._sc._jvm, \\\"org.apache.spark.sql.api.python.*\\\")\\r\\n\\r\\noutput_dataset \\u003d \\\"bad_records_output_1\\\"\\r\\n\\r\\nconn_params \\u003d {\\r\\n    \\\"host\\\": \\\"10.0.1.132\\\",\\r\\n    \\\"port\\\": 5432,\\r\\n    \\\"database\\\": \\\"poc\\\",\\r\\n    \\\"user\\\": \\\"postgres\\\",\\r\\n    \\\"password\\\": \\\"postgres\\\"\\r\\n}\\r\\n\\r\\n# ---------------- PostgreSQL Queries ---------------- #\\r\\nquery \\u003d \\\"\\\"\\\"\\r\\nWITH TableRowCounts AS (\\r\\n    SELECT \\r\\n        dt.table_id,\\r\\n        dt.schema_name,\\r\\n        dt.table_name,\\r\\n        c.reltuples::bigint AS current_row_count\\r\\n    FROM \\r\\n        t_dm_table dt\\r\\n    JOIN \\r\\n        pg_class c ON c.relname \\u003d dt.table_name\\r\\n    JOIN \\r\\n        pg_namespace n ON n.oid \\u003d c.relnamespace AND n.nspname \\u003d dt.schema_name\\r\\n)\\r\\nSELECT \\r\\n    dr_run.data_model_rule_run_id,\\r\\n    dr_run.bad_records_count,\\r\\n    dt.schema_name,\\r\\n    dt.table_name,\\r\\n    dr.column_id,\\r\\n    dr.rule_name,\\r\\n    dr_run.start_time,\\r\\n    dr_run.end_time\\r\\nFROM \\r\\n    t_dm_data_rule_run dr_run\\r\\nJOIN \\r\\n    t_dm_data_rule dr ON dr_run.data_model_rule_id \\u003d dr.data_rule_id\\r\\nJOIN \\r\\n    t_dm_table dt ON dr.table_id \\u003d dt.table_id\\r\\nLEFT JOIN\\r\\n    TableRowCounts trc ON dt.table_id \\u003d trc.table_id\\r\\nWHERE \\r\\n    dr_run.bad_records_count \\u003e 0\\r\\nORDER BY\\r\\n    dr_run.start_time DESC;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\ndeletion_query \\u003d \\\"\\\"\\\"\\r\\nSELECT schema_name, table_name, is_deleted, deleted_level\\r\\nFROM t_dm_TABLE\\r\\nWHERE is_deleted IS NOT NULL;\\r\\n\\\"\\\"\\\"\\r\\n\\r\\n# ---------------- Load Data and Spark DF ---------------- #\\r\\nwith psycopg2.connect(**conn_params) as conn:\\r\\n    df \\u003d pd.read_sql(query, conn)\\r\\n    deletion_df \\u003d pd.read_sql(deletion_query, conn)\\r\\n    df \\u003d df.merge(deletion_df, on\\u003d[\\u0027schema_name\\u0027, \\u0027table_name\\u0027], how\\u003d\\u0027left\\u0027)\\r\\n    df[\\u0027is_deleted\\u0027] \\u003d df[\\u0027is_deleted\\u0027].where(pd.notnull(df[\\u0027is_deleted\\u0027]), None)\\r\\n    df[\\u0027deleted_level\\u0027] \\u003d df[\\u0027deleted_level\\u0027].where(pd.notnull(df[\\u0027deleted_level\\u0027]), None)\\r\\n    spark_df \\u003d spark.createDataFrame(df)\\r\\n    spark_df.createOrReplaceTempView(output_dataset)\\r\\n    spark_df.show(25)\\r\\n\\r\\n# Save dataset in Datagaps engine\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.saveDataset(\\r\\n        spark_df._jdf, output_dataset, \\u0027$[component_name]\\u0027, $[dataflow_run_id]\\r\\n)\\r\\nspark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.deleteDatasetInfo(\\r\\n    json.dumps(output_dataset), $[dataflow_run_id], \\\"$[component_name]\\\"\\r\\n)\\r\\n\\r\\n# ---------------- SharePoint Upload Logic ---------------- #\\r\\ndef getSharepointDetails():\\r\\n    conn_map \\u003d spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getConnectionParams(\\r\\n        \\\"CSV_share_point\\\", \\\"read\\\", $[container_id]\\r\\n    )\\r\\n    conn \\u003d [conn_map.toList().apply(i) for i in range(conn_map.size())]\\r\\n    conn_details \\u003d {i._1(): i._2() for i in conn}\\r\\n    return {\\r\\n        \\\"tenantId\\\": conn_details[\\u0027tenantId\\u0027],\\r\\n        \\\"clientId\\\": conn_details[\\u0027clientId\\u0027],\\r\\n        \\\"clientSecret\\\": spark.sparkContext._jvm.com.datagaps.core.engine.utils.CodeUtils.getDecryptedPassword(\\r\\n            conn_details[\\u0027clientSecret\\u0027]\\r\\n        ),\\r\\n        \\\"folderPath\\\": conn_details[\\u0027path\\u0027],       # SharePoint folder path\\r\\n        \\\"siteSearch\\\": conn_details[\\u0027siteSearch\\u0027],\\r\\n        \\\"grantType\\\": \\\"client_credentials\\\"\\r\\n    }\\r\\n\\r\\ndef get_access_token(config):\\r\\n    url \\u003d f\\\"https://login.microsoftonline.com/{config[\\u0027tenantId\\u0027]}/oauth2/token\\\"\\r\\n    payload \\u003d {\\r\\n        \\\"grant_type\\\": config[\\\"grantType\\\"],\\r\\n        \\\"client_id\\\": config[\\\"clientId\\\"],\\r\\n        \\\"client_secret\\\": config[\\\"clientSecret\\\"],\\r\\n        \\\"resource\\\": \\\"https://graph.microsoft.com/\\\"\\r\\n    }\\r\\n    response \\u003d requests.post(url, data\\u003dpayload)\\r\\n    response.raise_for_status()\\r\\n    return response.json()[\\\"access_token\\\"]\\r\\n\\r\\ndef get_site_id(token, site_search):\\r\\n    url \\u003d f\\\"https://graph.microsoft.com/v1.0/sites?search\\u003d{site_search}\\\"\\r\\n    headers \\u003d {\\\"Authorization\\\": f\\\"Bearer {token}\\\"}\\r\\n    response \\u003d requests.get(url, headers\\u003dheaders)\\r\\n    response.raise_for_status()\\r\\n    sites \\u003d response.json().get(\\\"value\\\", [])\\r\\n    if not sites:\\r\\n        raise Exception(f\\\"No site found for search: {site_search}\\\")\\r\\n    return sites[0][\\\"id\\\"]\\r\\n\\r\\ndef upload_file_to_sharepoint(token, site_id, folder_path, filename, file_bytes):\\r\\n    folder_path_encoded \\u003d quote(folder_path.strip(\\\"/\\\"))\\r\\n    upload_url \\u003d f\\\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root:/{folder_path}/{filename}:/content\\\"\\r\\n    headers \\u003d {\\\"Authorization\\\": f\\\"Bearer {token}\\\", \\\"Content-Type\\\": \\\"application/octet-stream\\\"}\\r\\n    response \\u003d requests.put(upload_url, headers\\u003dheaders, data\\u003dfile_bytes)\\r\\n    response.raise_for_status()\\r\\n    file_info \\u003d response.json()\\r\\n    print(f\\\"File uploaded successfully to \\u0027{folder_path}/{filename}\\u0027\\\")\\r\\n    return file_info[\\\"id\\\"], file_info[\\\"webUrl\\\"]\\r\\n\\r\\n# ---------------- Execute Upload ---------------- #\\r\\nsp_config \\u003d getSharepointDetails()\\r\\naccess_token \\u003d get_access_token(sp_config)\\r\\nsite_id \\u003d get_site_id(access_token, sp_config[\\\"siteSearch\\\"])\\r\\n\\r\\n# Convert Spark DF to in-memory CSV\\r\\ncsv_buffer \\u003d io.BytesIO()\\r\\npdf \\u003d spark_df.toPandas()\\r\\npdf.to_csv(csv_buffer, index\\u003dFalse)\\r\\ncsv_bytes \\u003d csv_buffer.getvalue()\\r\\n\\r\\ncsv_filename \\u003d \\\"bad_records_output.csv\\\"\\r\\nfile_id, file_url \\u003d upload_file_to_sharepoint(access_token, site_id, sp_config[\\\"folderPath\\\"], csv_filename, csv_bytes)\\r\\n\\r\\nprint(\\\"File ID:\\\", file_id)\\r\\nprint(\\\"File URL:\\\", file_url)\\r\\n\",\"kind\":\"pyspark\",\"dataSourceId\":0,\"hideDatasets\":\"N\",\"componentId\":8,\"componentName\":\"Code 8\",\"tableName\":\"\",\"category\":\"Processor\",\"componentType\":\"Code\",\"rank\":0,\"dataSourceName\":\"\",\"displayRows\":50,\"dependencies\":[],\"options\":{},\"className\":\"com.datagaps.dataflow.models.CodeComponent\",\"executionOption\":\"\",\"excludeNotification\":\"N\"}],\"isDeleteWorkSchemaTable\":\"N\"}","parameters":"[{\"type\":\"static\",\"name\":\"limit_rows\",\"defValueInInteractiveMode\":\"limit 10\",\"defValueInBatchMode\":\"limit 1000\"}]","version":3,"maxComponentId":8,"livyOptions":null,"isDeleted":"N","userName":null,"type":"dataflow","environmentName":null,"folderPath":"Dataflow","workSchemaName":null,"diagramSchema":"{\"isCustom\":false,\"diagramDef\":[]}"},"analysis":[],"datamodels":[],"tagDetails":[],"dataCompares":[],"filterDatasetMappings":[],"referenceDatasets":null}